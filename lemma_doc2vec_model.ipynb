{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f62078",
   "metadata": {},
   "source": [
    "Another way of creating sentence vectors for captions is by using Gensim's Doc2Vec model which creates document vectors for each caption. For this notebook, we are going to lemmatize our preprocessed words compared to the other Doc2Vec notebook which contains no lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0371ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for the doc2vec model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import re\n",
    "from collections import defaultdict \n",
    "from numpy import dot\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ae3fb",
   "metadata": {},
   "source": [
    "I connected to a MySQL database called 'new_york_cartoon'. I pulled down the relevant data which included the caption text in 'caption' column and their respective rankings in 'ranking' column into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to SQL database\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "try:\n",
    "    connection = mysql.connector.connect(host='dbnewyorkcartoon.cgyqzvdc98df.us-east-2.rds.amazonaws.com',\n",
    "                                         database='new_york_cartoon',\n",
    "                                         user='dbuser',\n",
    "                                         password='Sql123456')\n",
    "    if connection.is_connected():\n",
    "        db_Info = connection.get_server_info()\n",
    "        print(\"Connected to MySQL Server version \", db_Info)\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"select database();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(\"You succeed to connect to database: \", record)\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d979c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling down data from SQL database via search\n",
    "sql_select_Query = \"select caption,ranking from result;\"  # you can change query in this line for selecting your target data\n",
    "cursor.execute(sql_select_Query)\n",
    "\n",
    "# show attributes names of target data\n",
    "num_attr = len(cursor.description)\n",
    "attr_names = [i[0] for i in cursor.description]\n",
    "print(attr_names)\n",
    "\n",
    "# get all records\n",
    "records = cursor.fetchall()\n",
    "print(\"Total number of rows in table: \", cursor.rowcount)\n",
    "df = pd.DataFrame(records, columns=attr_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9bcdea",
   "metadata": {},
   "source": [
    "I am dropping the 'ranking' column because we are not using numbers as they contain unnecessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ff6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneccessary columns, axis = 1 means to remove vertical axis(columns)\n",
    "df = df.drop(columns=['ranking'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f8f1f",
   "metadata": {},
   "source": [
    "I am removing punctuation to remove noise and lowercasing words for a uniform word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation lowercasing and creating new column \"caption_processed\"\n",
    "df['caption'] = df['caption'].astype(str)\n",
    "df['caption_processed'] = df['caption'].map(lambda x: re.sub(r'[,\\.\\!\\?\\'\\\"]', '', x).lower())\n",
    "df['caption_processed'] = df['caption_processed'].map(lambda x: re.sub(r'[--]', ' ', x).lower())\n",
    "\n",
    "# Print out the first rows of captions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642e309",
   "metadata": {},
   "source": [
    "I use the gensim function of simple_preprocess to tokenize the words and deaccent all words to create a uniform list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing and cleaning up text\n",
    "data = df.caption_processed.values.tolist()\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e107e",
   "metadata": {},
   "source": [
    "Within the data, there might be brigrams and trigrams. So I built bigrams and trigrams for words that appear a minimum of 5 times together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42757ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold = fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fffda05",
   "metadata": {},
   "source": [
    "I chose Spacy's library of stopwords because it is larger than NLTK's library and may help reduce noise even further for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading stopwords from Spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d113ff",
   "metadata": {},
   "source": [
    "I created functions to remove stop words, find bigram phrases, trigram phrases, and lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318daba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6112d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv, propn\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN'])\n",
    "print(data_lemmatized[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a4c4c",
   "metadata": {},
   "source": [
    "Here I use the function, 'TaggedDocument', to create unique tags for each caption. Each caption will be seperate from one another when creating vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0914e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating document ids\n",
    "def tagged_document(documents):\n",
    "    for i, words in enumerate(documents):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(words, [i])\n",
    "\n",
    "tagged_documents = list(tagged_document(documents))\n",
    "\n",
    "# Print the first TaggedDocument\n",
    "print(tagged_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4997992",
   "metadata": {},
   "source": [
    "If you wish to see how the raw vectors look like, please uncomment this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd814f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, tagged_doc in enumerate(d_vectors):\n",
    "    # words = tagged_doc.words\n",
    "    # print(f\"Words in Document {i}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a0834",
   "metadata": {},
   "source": [
    "I created the Doc2Vec model with the window size being 10 to encompass semantic meaning. Additionally, I am using 'dm=1' and 'dbow_words=0' to use distributed memory training for more coherent semnatic meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 200, \n",
    "                                      window = 10,\n",
    "                                      min_count = 5, \n",
    "                                      dm = 1,\n",
    "                                      dbow_words = 0,\n",
    "                                      epochs = 15,\n",
    "                                      workers = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model vocabulary\n",
    "model.build_vocab(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4bf93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05417dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving caption vectors to a compressed numpy file\n",
    "def get_document_vectors_with_ids(model, tagged_docs):\n",
    "    document_vectors = []\n",
    "    for i, (doc_id, dv) in enumerate(zip(tagged_docs, model.dv)):\n",
    "        words = doc_id.words\n",
    "        document_vectors.append((f\"Document {i + 1}\", words, dv))\n",
    "    return document_vectors\n",
    "\n",
    "document_vectors_with_ids = get_document_vectors_with_ids(model, tagged_documents)\n",
    "\n",
    "dtype = [('doc_id', 'U20'), ('words', object), ('doc_vector', np.float32, (model.vector_size,))]\n",
    "data = np.array(document_vectors_with_ids, dtype=dtype)\n",
    "\n",
    "# Save the data as an NPZ file\n",
    "np.savez(\"caption_vectors_lemma.npz\", data=data)\n",
    "\n",
    "print(\"Document IDs, vectors, and words saved to 'caption_vectors_lemma.npz'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
